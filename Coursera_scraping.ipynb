{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Courses file uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "file_path = r\"Formations_Cours.csv\"\n",
    "courses_df = pd.read_csv(file_path)\n",
    "courses_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coursera site scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the Chrome driver\n",
    "path = '/Users/yhema/Downloads/chromedriver-win64/chromedriver-win64/chromedriver.exe'\n",
    "service = Service(executable_path=path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Load the file with domain, sous-domain, and course details\n",
    "file_path = r\"E:\\M2\\BDA\\SEFORMER_FORMATIONS_PAS_DE_FORMATEURS.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Prepare the CSV file for writing results\n",
    "output_file = \"course_links_with_titles_cont.csv\"\n",
    "\n",
    "with open(output_file, mode=\"w\", newline='', encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Domain\", \"Sous-Domain\", \"Course Name\", \"Title 1\", \"Link 1\", \"Title 2\", \"Link 2\", \"Title 3\", \"Link 3\"])\n",
    "\n",
    "    # Loop through each row to build the Coursera URL and extract course titles and links (up to 5 rows)\n",
    "    for _ , row in data.iterrows():\n",
    "        \n",
    "        domain = row['DOMAINE_CATGEGORIE']\n",
    "        sous_domain = row['SOUS_DOMAINE_CATEGORIE']\n",
    "        course_name = row['FORMATION_COURS']\n",
    "\n",
    "        # Construct Coursera search URL\n",
    "        coursera_search_url = f\"https://www.coursera.org/search?query={domain}+{sous_domain}+{course_name}\"\n",
    "        driver.get(coursera_search_url)\n",
    "\n",
    "        # Accept cookies if needed (optional)\n",
    "        try:\n",
    "            accept_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//button[contains(text(),\"Accept\")]'))\n",
    "            )\n",
    "            accept_button.click()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Extract the top 3 course titles and links\n",
    "        titles_links = []\n",
    "        try:\n",
    "            course_elements = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, '//a[contains(@class,\"cds-CommonCard-titleLink\")]'))\n",
    "            )\n",
    "\n",
    "            for i, course_element in enumerate(course_elements[:3]):\n",
    "                title = course_element.text\n",
    "                link = course_element.get_attribute(\"href\")\n",
    "                titles_links.append((title, link))\n",
    "\n",
    "            # Fill missing entries if less than 3 courses are found\n",
    "            while len(titles_links) < 3:\n",
    "                titles_links.append((\"Not found\", \"Not found\"))\n",
    "\n",
    "            # Write the row with domain, sous-domain, course name, and the 3 titles and links\n",
    "            row_data = [domain, sous_domain, course_name]\n",
    "            for title, link in titles_links:\n",
    "                row_data.extend([title, link])\n",
    "\n",
    "            writer.writerow(row_data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting data for '{domain} - {sous_domain} - {course_name}': {e}\")\n",
    "\n",
    "print(f\"Course links and titles saved to {output_file}\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
